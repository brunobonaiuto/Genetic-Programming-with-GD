# Genetic-Programming-with-Gradient-Descent
This work describe different approaches to use gradient descent in
genetic programming (GP), performing different methods. The results
show when is profitable to apply GP with Gradient Descent and also
when is not statistically significant.
- Genetic Algorithms are evolutionary search techniques inspired by natural selection
(i.e survival of the fittest), that works with a ”population” of trial solutions
to a problem, frequently encoded as strings, and repeatedly select the ”fitter”
solutions, in this way to evolve better ones. The power of GAs is being demonstrated
for an increasing range of applications. But one of the most intriguing
uses of GAs - by Koza is, automatic program generation.
- Genetic Programming applies Genetic algorithms to a ”population” of programs 
typically encoded as tree-structures. Trial programs are evaluated
against a ”fitness function” and the best solutions selected for modification and
re evaluation. This modification-evaluation cycle is repeated until a ”correct”
program is produced. GP has demonstrated its potential by evolving simple
programs for medical signal filters, classifying news stories, performing optical
character recognition, and for target identification.
- Gradient descent is optimization algorithm commonly used as black-box optimizer,
in this work it will applied as a hybrid approach with GP. In order to
search for improvements in the GP algorithms.
